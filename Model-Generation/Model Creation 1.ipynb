{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import subprocess\n",
        "\n",
        "\n",
        "#command = \"./genModelPrereqs.sh /home/jaredrussell/CPPDataset/ /home/jaredrussell/CPPMLGen/\"\n",
        "#subprocess.run(command, shell=True, cwd=\"/home/jaredrussell/gitRepos/SCA-ML-grade/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BFlZYHLP1AyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def homePath(path):\n",
        "    if path[0] == \"~\":\n",
        "        return os.path.join(os.path.expanduser(\"~\"), path.strip(\"~/\"))\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "#Model Creation 1\n",
        "#Figure out how to do versioning effectively\n",
        "individualInput = False\n",
        "combinedInputPath = \"/home/jaredrussell/MLGen\"\n",
        "#combinedInputPath = \"/home/jaredrussell/CPPMLGenMini\"\n",
        "pathToTokenizedData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Tokenizer\\\\\"\n",
        "pathToGradeData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Grader\\\\\"\n",
        "GradesTokensName = \"\"\n",
        "\n",
        "\n",
        "if not individualInput:\n",
        "    pathToTokenizedData = os.path.join(combinedInputPath, \"Tokens/\")\n",
        "    pathToGradeData = os.path.join(combinedInputPath, \"Grades/\")\n",
        "\n",
        "\n",
        "if GradesTokensName == \"\":\n",
        "    #Use newest folder for each\n",
        "\n",
        "    #Get the newest folder for the tokens\n",
        "    tokensFolders = os.listdir(pathToTokenizedData)\n",
        "    tokensFolders.sort()\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData , tokensFolders[-1])\n",
        "\n",
        "    #Get the newest folder for the grades\n",
        "    gradesFolders = os.listdir(pathToGradeData)\n",
        "    gradesFolders.sort()\n",
        "    pathToGradeData = os.path.join(pathToGradeData,  gradesFolders[-1])\n",
        "else:\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData, GradesTokensName)\n",
        "    pathToGradeData = os.path.join(pathToGradeData, GradesTokensName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelOutputPath = \"/home/jaredrussell/MLGen/Models\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nLHjdZFH05pS",
        "outputId": "1016b38b-0f18-42e3-8e28-1e03019b46a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 15:11:57.459812: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-11 15:11:57.510970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-11 15:11:57.511006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-11 15:11:57.512310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-11 15:11:57.519936: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-12-11 15:11:57.520811: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-11 15:11:58.391619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "3       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "4       [6, 41, 5, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "...                                                   ...   \n",
            "178097  [6, 1, 6, 1, 25, 1, 28, 26, 14, 8, 62, 11, 13,...   \n",
            "178098  [6, 41, 10, 14, 9, 8, 41, 3, 1, 20, 16, 21, 32...   \n",
            "178099  [6, 80, 6, 29, 3, 45, 12, 1, 3, 1, 6, 1, 12, 1...   \n",
            "178100  [6, 80, 6, 29, 12, 1, 3, 68, 6, 1, 5, 1, 22, 1...   \n",
            "178101  [6, 80, 12, 1, 3, 68, 6, 1, 5, 1, 22, 18, 14, ...   \n",
            "\n",
            "                                                   Path  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py  \n",
            "2                              3912958/doc/conf.py/2.py  \n",
            "3                              3912958/doc/conf.py/4.py  \n",
            "4                              3912958/doc/conf.py/0.py  \n",
            "...                                                 ...  \n",
            "178097     3913980/tests/wd/helloworld_pb2_grpc.py/0.py  \n",
            "178098       3913980/tests/wd/protos/sample_pb2.py/0.py  \n",
            "178099                3913980/tests/test_config.py/2.py  \n",
            "178100                3913980/tests/test_config.py/4.py  \n",
            "178101                3913980/tests/test_config.py/3.py  \n",
            "\n",
            "[178102 rows x 2 columns]\n",
            "        fileGrade                                             Path\n",
            "0            30.8  3912958/doc/_themes/flask_theme_support.py/0.py\n",
            "1            40.8  3912958/doc/_themes/flask_theme_support.py/1.py\n",
            "2            30.8                         3912958/doc/conf.py/0.py\n",
            "3            33.3                         3912958/doc/conf.py/1.py\n",
            "4            35.8                         3912958/doc/conf.py/2.py\n",
            "...           ...                                              ...\n",
            "278330       50.6                3913980/tests/test_config.py/0.py\n",
            "278331       57.6                3913980/tests/test_config.py/1.py\n",
            "278332       58.6                3913980/tests/test_config.py/2.py\n",
            "278333       62.6                3913980/tests/test_config.py/3.py\n",
            "278334       66.6                3913980/tests/test_config.py/4.py\n",
            "\n",
            "[278335 rows x 2 columns]\n",
            "                                                tokenCode  \\\n",
            "0       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "3       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "4       [6, 41, 5, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "...                                                   ...   \n",
            "177896  [6, 1, 6, 1, 25, 1, 28, 26, 14, 8, 62, 11, 13,...   \n",
            "177897  [6, 41, 10, 14, 9, 8, 41, 3, 1, 20, 16, 21, 32...   \n",
            "177898  [6, 80, 6, 29, 3, 45, 12, 1, 3, 1, 6, 1, 12, 1...   \n",
            "177899  [6, 80, 6, 29, 12, 1, 3, 68, 6, 1, 5, 1, 22, 1...   \n",
            "177900  [6, 80, 12, 1, 3, 68, 6, 1, 5, 1, 22, 18, 14, ...   \n",
            "\n",
            "                                                   Path  fileGrade  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py       30.8  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py       40.8  \n",
            "2                              3912958/doc/conf.py/2.py       35.8  \n",
            "3                              3912958/doc/conf.py/4.py       40.8  \n",
            "4                              3912958/doc/conf.py/0.py       30.8  \n",
            "...                                                 ...        ...  \n",
            "177896     3913980/tests/wd/helloworld_pb2_grpc.py/0.py       53.6  \n",
            "177897       3913980/tests/wd/protos/sample_pb2.py/0.py       50.6  \n",
            "177898                3913980/tests/test_config.py/2.py       58.6  \n",
            "177899                3913980/tests/test_config.py/4.py       66.6  \n",
            "177900                3913980/tests/test_config.py/3.py       62.6  \n",
            "\n",
            "[177901 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('./SCA-Tokenizer/')\n",
        "import CLPTokenizer as CLPTokenizer\n",
        "\n",
        "\n",
        "\n",
        "#Load the data\n",
        "#tokenized data is in tokenizedData.pkl, has tokenizer obj in tokenizer.json\n",
        "\n",
        "#Load the tokenizer\n",
        "with open(pathToTokenizedData + \"/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f, encoding='latin1')\n",
        "\n",
        "\n",
        "#Load the tokenized data\n",
        "with open(pathToTokenizedData + \"/tokenizedData.pkl\", \"rb\") as f:\n",
        "    tokenizedData = pickle.load(f)\n",
        "\n",
        "print (tokenizedData)\n",
        "\n",
        "#Load the grade data\n",
        "#gradeData is a dict with keys as the file names and values as the grades\n",
        "with open(pathToGradeData + \"/grades.pkl\", \"rb\") as f:\n",
        "    gradeData = pickle.load(f)\n",
        "\n",
        "print (gradeData)\n",
        "\n",
        "#load the group data\n",
        "#with open(pathToTokenizedData + \"/tokenGroupDataframe.pkl\", \"rb\") as f:\n",
        "#    tokenizedGroupData = pickle.load(f)\n",
        "\n",
        "combinedDF = pd.merge(tokenizedData, gradeData, on = \"Path\")\n",
        "#combinedDF = pd.merge(combinedDF, tokenizedGroupData, on = \"Path\")\n",
        "print (combinedDF)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jnja4-p705pU",
        "outputId": "b79f4a7d-57af-4836-e052-f2089eaec5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 3, 1, 6, 1, 12, 1, 3, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "3       [6, 41, 6, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "4       [6, 41, 5, 29, 41, 3, 45, 3, 1, 8, 16, 5, 29, ...   \n",
            "...                                                   ...   \n",
            "177896  [6, 1, 6, 1, 25, 1, 28, 26, 14, 8, 62, 11, 13,...   \n",
            "177897  [6, 41, 10, 14, 9, 8, 41, 3, 1, 20, 16, 21, 32...   \n",
            "177898  [6, 80, 6, 29, 3, 45, 12, 1, 3, 1, 6, 1, 12, 1...   \n",
            "177899  [6, 80, 6, 29, 12, 1, 3, 68, 6, 1, 5, 1, 22, 1...   \n",
            "177900  [6, 80, 12, 1, 3, 68, 6, 1, 5, 1, 22, 18, 14, ...   \n",
            "\n",
            "                                                   Path  fileGrade  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py       30.8  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py       40.8  \n",
            "2                              3912958/doc/conf.py/2.py       35.8  \n",
            "3                              3912958/doc/conf.py/4.py       40.8  \n",
            "4                              3912958/doc/conf.py/0.py       30.8  \n",
            "...                                                 ...        ...  \n",
            "177896     3913980/tests/wd/helloworld_pb2_grpc.py/0.py       53.6  \n",
            "177897       3913980/tests/wd/protos/sample_pb2.py/0.py       50.6  \n",
            "177898                3913980/tests/test_config.py/2.py       58.6  \n",
            "177899                3913980/tests/test_config.py/4.py       66.6  \n",
            "177900                3913980/tests/test_config.py/3.py       62.6  \n",
            "\n",
            "[177901 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#Padding\n",
        "\n",
        "maxLen = 500\n",
        "minLen = 100\n",
        "#get rid of the ones that are too long\n",
        "\n",
        "#print (type(combinedDF[\"tokenCode\"]))\n",
        "combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) <= maxLen -1]\n",
        "\n",
        "#shorten the ones that are too long FOR TESTING\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()[:maxLen]])\n",
        "\n",
        "#get rid of the ones that are too short DISABLED FOR TESTING\n",
        "#combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) > minLen]\n",
        "\n",
        "#Pad the sequences\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()]).tolist()\n",
        "combinedDF[\"tokenCode\"] = pad_sequences(combinedDF[\"tokenCode\"].tolist(), maxlen=maxLen, padding=\"post\", truncating=\"post\").tolist()\n",
        "#combinedDF[\"tokenGroupCode\"] = pad_sequences(combinedDF[\"tokenGroupCode\"], maxlen = maxLen, padding = \"post\", truncating = \"post\").tolist()\n",
        "\n",
        "print(combinedDF)\n",
        "\n",
        "#only use 5% of the data\n",
        "#combinedDF = combinedDF.sample(frac=0.001, random_state=1)\n",
        "\n",
        "#48590 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Nu6hyuey05pU",
        "outputId": "2d9cf582-6a4a-46ad-e21c-e84a8f2eecce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "301\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 15:12:10.978525: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 711604000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5560/5560 [==============================] - 126s 22ms/step - loss: 0.3319 - mse: 308.2231 - mae: 13.6379 - mape: 202906960.0000 - accuracy: 9.5559e-05\n",
            "Epoch 2/3\n",
            "5560/5560 [==============================] - 119s 21ms/step - loss: 0.1285 - mse: 141.1283 - mae: 8.8534 - mape: 78589688.0000 - accuracy: 1.1242e-05\n",
            "Epoch 3/3\n",
            "5560/5560 [==============================] - 120s 22ms/step - loss: 0.1125 - mse: 122.9474 - mae: 8.1848 - mape: 63987604.0000 - accuracy: 1.6863e-05\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.gitignore\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/obfuscate.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/TokenizerManager.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/test.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.DS_Store\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/main.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.git\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/PythonProcessing.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/CodeSimilarization.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/TokenProcessing.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/CodeSimilarization.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/PythonProcessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/TokenProcessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPTokenizer.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/obfuscate.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPPreprocessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CLPTokenizer.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/Grader.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/Grader.cpython-310.pyc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "tar: Removing leading `/' from member names\n",
            "tar: Removing leading `/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../' from member names\n"
          ]
        }
      ],
      "source": [
        "#if (not gpu_detected):\n",
        "#    print(\"GPU not detected, using CPU\")\n",
        "\n",
        "number_of_tokens = len(tokenizer.word_index) + 1\n",
        "print (number_of_tokens)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(number_of_tokens, 64, input_length=maxLen))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Reshape((1, 64)))\n",
        "model.add(layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.38479930887149405))\n",
        "model.add(layers.Bidirectional(LSTM(32)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='Adam', metrics=['mse', 'mae', 'mape', 'accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert inputs to numpy arrays\n",
        "token_code = np.array(combinedDF[\"tokenCode\"].tolist())\n",
        "#token_group_code = np.array(combinedDF[\"tokenGroupCode\"].tolist())\n",
        "file_grade = np.array(combinedDF[\"fileGrade\"].tolist())\n",
        "\n",
        "hist = model.fit(token_code, file_grade, epochs=3, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "#Save the model in timestamp folder and with tokenizer\n",
        "timestamp = str(pd.Timestamp.now()).replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
        "if not os.path.exists(modelOutputPath):\n",
        "    os.mkdir(modelOutputPath)\n",
        "if not os.path.exists(modelOutputPath + \"/\" + timestamp):\n",
        "    os.mkdir(modelOutputPath + \"/\" + timestamp)\n",
        "model.save(modelOutputPath + \"/\" + timestamp + \"/model.h5\")\n",
        "with open(modelOutputPath + \"/\" + timestamp + \"/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "#make an archaive of the SCA-Tokenizer Folder\n",
        "#get CWD\n",
        "cwd = os.getcwd()\n",
        "if os.path.exists(os.path.join(cwd, \"SCA-Tokenizer\")):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/SCA-Tokenizer.tar.gz\\\" \" + os.path.join(cwd, \"SCA-Tokenizer\"))\n",
        "\n",
        "#Save the AutoGrader Folder\n",
        "autoGraderDir = os.path.join(cwd, \"../Auto-Grader/\")\n",
        "if os.path.exists(autoGraderDir):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/Auto-Grader.tar.gz\\\" \" + autoGraderDir)\n",
        "\n",
        "#Save tokenizedGroupData\n",
        "#with open(modelOutputPath + \"/\" + timestamp + \"/tokenizedGroupDataframe.pkl\", \"wb\") as f:\n",
        "#    pickle.dump(tokenizedGroupData, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
