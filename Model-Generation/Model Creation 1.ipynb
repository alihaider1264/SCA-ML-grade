{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import subprocess\n",
        "\n",
        "\n",
        "#command = \"./genModelPrereqs.sh /home/jaredrussell/CPPDataset/ /home/jaredrussell/CPPMLGen/\"\n",
        "#subprocess.run(command, shell=True, cwd=\"/home/jaredrussell/gitRepos/SCA-ML-grade/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BFlZYHLP1AyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def homePath(path):\n",
        "    if path[0] == \"~\":\n",
        "        return os.path.join(os.path.expanduser(\"~\"), path.strip(\"~/\"))\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "#Model Creation 1\n",
        "#Figure out how to do versioning effectively\n",
        "individualInput = False\n",
        "combinedInputPath = \"/home/jaredrussell/MLGen\"\n",
        "#combinedInputPath = \"/home/jaredrussell/CPPMLGenMini\"\n",
        "pathToTokenizedData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Tokenizer\\\\\"\n",
        "pathToGradeData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Grader\\\\\"\n",
        "GradesTokensName = \"\"\n",
        "\n",
        "\n",
        "if not individualInput:\n",
        "    pathToTokenizedData = os.path.join(combinedInputPath, \"Tokens/\")\n",
        "    pathToGradeData = os.path.join(combinedInputPath, \"Grades/\")\n",
        "\n",
        "\n",
        "if GradesTokensName == \"\":\n",
        "    #Use newest folder for each\n",
        "\n",
        "    #Get the newest folder for the tokens\n",
        "    tokensFolders = os.listdir(pathToTokenizedData)\n",
        "    tokensFolders.sort()\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData , tokensFolders[-1])\n",
        "\n",
        "    #Get the newest folder for the grades\n",
        "    gradesFolders = os.listdir(pathToGradeData)\n",
        "    gradesFolders.sort()\n",
        "    pathToGradeData = os.path.join(pathToGradeData,  gradesFolders[-1])\n",
        "else:\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData, GradesTokensName)\n",
        "    pathToGradeData = os.path.join(pathToGradeData, GradesTokensName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelOutputPath = \"/home/jaredrussell/MLGen/Models\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nLHjdZFH05pS",
        "outputId": "1016b38b-0f18-42e3-8e28-1e03019b46a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "3       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "4       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "...                                                   ...   \n",
            "248790  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "248791  [6, 80, 6, 30, 12, 1, 4, 74, 6, 1, 5, 1, 23, 1...   \n",
            "248792  [6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 23, 18, 14, ...   \n",
            "248793  [6, 80, 12, 1, 4, 74, 6, 1, 5, 1, 23, 18, 14, ...   \n",
            "248794  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "\n",
            "                                                   Path  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py  \n",
            "2                              3912958/doc/conf.py/2.py  \n",
            "3                              3912958/doc/conf.py/4.py  \n",
            "4                              3912958/doc/conf.py/6.py  \n",
            "...                                                 ...  \n",
            "248790                3913980/tests/test_config.py/2.py  \n",
            "248791                3913980/tests/test_config.py/4.py  \n",
            "248792                3913980/tests/test_config.py/0.py  \n",
            "248793                3913980/tests/test_config.py/3.py  \n",
            "248794                3913980/tests/test_config.py/1.py  \n",
            "\n",
            "[248795 rows x 2 columns]\n",
            "        fileGrade                                             Path\n",
            "0            30.8  3912958/doc/_themes/flask_theme_support.py/0.py\n",
            "1            40.8  3912958/doc/_themes/flask_theme_support.py/1.py\n",
            "2            30.8                         3912958/doc/conf.py/0.py\n",
            "3            33.3                         3912958/doc/conf.py/1.py\n",
            "4            35.8                         3912958/doc/conf.py/2.py\n",
            "...           ...                                              ...\n",
            "278330       50.6                3913980/tests/test_config.py/0.py\n",
            "278331       57.6                3913980/tests/test_config.py/1.py\n",
            "278332       58.6                3913980/tests/test_config.py/2.py\n",
            "278333       62.6                3913980/tests/test_config.py/3.py\n",
            "278334       66.6                3913980/tests/test_config.py/4.py\n",
            "\n",
            "[278335 rows x 2 columns]\n",
            "                                                tokenCode  \\\n",
            "0       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "3       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "4       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "...                                                   ...   \n",
            "248514  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "248515  [6, 80, 6, 30, 12, 1, 4, 74, 6, 1, 5, 1, 23, 1...   \n",
            "248516  [6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 23, 18, 14, ...   \n",
            "248517  [6, 80, 12, 1, 4, 74, 6, 1, 5, 1, 23, 18, 14, ...   \n",
            "248518  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "\n",
            "                                                   Path  fileGrade  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py       30.8  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py       40.8  \n",
            "2                              3912958/doc/conf.py/2.py       35.8  \n",
            "3                              3912958/doc/conf.py/4.py       40.8  \n",
            "4                              3912958/doc/conf.py/6.py       45.8  \n",
            "...                                                 ...        ...  \n",
            "248514                3913980/tests/test_config.py/2.py       58.6  \n",
            "248515                3913980/tests/test_config.py/4.py       66.6  \n",
            "248516                3913980/tests/test_config.py/0.py       50.6  \n",
            "248517                3913980/tests/test_config.py/3.py       62.6  \n",
            "248518                3913980/tests/test_config.py/1.py       57.6  \n",
            "\n",
            "[248519 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('./SCA-Tokenizer/')\n",
        "import CLPTokenizer as CLPTokenizer\n",
        "\n",
        "\n",
        "\n",
        "#Load the data\n",
        "#tokenized data is in tokenizedData.pkl, has tokenizer obj in tokenizer.json\n",
        "\n",
        "#Load the tokenizer\n",
        "with open(pathToTokenizedData + \"/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f, encoding='latin1')\n",
        "\n",
        "\n",
        "#Load the tokenized data\n",
        "with open(pathToTokenizedData + \"/tokenizedData.pkl\", \"rb\") as f:\n",
        "    tokenizedData = pickle.load(f)\n",
        "\n",
        "print (tokenizedData)\n",
        "\n",
        "#Load the grade data\n",
        "#gradeData is a dict with keys as the file names and values as the grades\n",
        "with open(pathToGradeData + \"/grades.pkl\", \"rb\") as f:\n",
        "    gradeData = pickle.load(f)\n",
        "\n",
        "print (gradeData)\n",
        "\n",
        "#load the group data\n",
        "#with open(pathToTokenizedData + \"/tokenGroupDataframe.pkl\", \"rb\") as f:\n",
        "#    tokenizedGroupData = pickle.load(f)\n",
        "\n",
        "combinedDF = pd.merge(tokenizedData, gradeData, on = \"Path\")\n",
        "#combinedDF = pd.merge(combinedDF, tokenizedGroupData, on = \"Path\")\n",
        "print (combinedDF)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jnja4-p705pU",
        "outputId": "b79f4a7d-57af-4836-e052-f2089eaec5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "1       [12, 1, 4, 1, 6, 1, 12, 1, 4, 1, 6, 1, 5, 1, 5...   \n",
            "2       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "3       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "4       [6, 44, 6, 30, 44, 4, 48, 4, 1, 8, 16, 5, 30, ...   \n",
            "...                                                   ...   \n",
            "248514  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "248515  [6, 80, 6, 30, 12, 1, 4, 74, 6, 1, 5, 1, 23, 1...   \n",
            "248516  [6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 23, 18, 14, ...   \n",
            "248517  [6, 80, 12, 1, 4, 74, 6, 1, 5, 1, 23, 18, 14, ...   \n",
            "248518  [6, 80, 6, 30, 4, 48, 12, 1, 4, 1, 6, 1, 12, 1...   \n",
            "\n",
            "                                                   Path  fileGrade  \n",
            "0       3912958/doc/_themes/flask_theme_support.py/0.py       30.8  \n",
            "1       3912958/doc/_themes/flask_theme_support.py/1.py       40.8  \n",
            "2                              3912958/doc/conf.py/2.py       35.8  \n",
            "3                              3912958/doc/conf.py/4.py       40.8  \n",
            "4                              3912958/doc/conf.py/6.py       45.8  \n",
            "...                                                 ...        ...  \n",
            "248514                3913980/tests/test_config.py/2.py       58.6  \n",
            "248515                3913980/tests/test_config.py/4.py       66.6  \n",
            "248516                3913980/tests/test_config.py/0.py       50.6  \n",
            "248517                3913980/tests/test_config.py/3.py       62.6  \n",
            "248518                3913980/tests/test_config.py/1.py       57.6  \n",
            "\n",
            "[248519 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#Padding\n",
        "\n",
        "maxLen = 500\n",
        "minLen = 100\n",
        "#get rid of the ones that are too long\n",
        "\n",
        "#print (type(combinedDF[\"tokenCode\"]))\n",
        "combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) <= maxLen -1]\n",
        "\n",
        "#shorten the ones that are too long FOR TESTING\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()[:maxLen]])\n",
        "\n",
        "#get rid of the ones that are too short DISABLED FOR TESTING\n",
        "#combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) > minLen]\n",
        "\n",
        "#Pad the sequences\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()]).tolist()\n",
        "combinedDF[\"tokenCode\"] = pad_sequences(combinedDF[\"tokenCode\"].tolist(), maxlen=maxLen, padding=\"post\", truncating=\"post\").tolist()\n",
        "#combinedDF[\"tokenGroupCode\"] = pad_sequences(combinedDF[\"tokenGroupCode\"], maxlen = maxLen, padding = \"post\", truncating = \"post\").tolist()\n",
        "\n",
        "print(combinedDF)\n",
        "\n",
        "#only use 5% of the data\n",
        "#combinedDF = combinedDF.sample(frac=0.001, random_state=1)\n",
        "\n",
        "#48590 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nu6hyuey05pU",
        "outputId": "2d9cf582-6a4a-46ad-e21c-e84a8f2eecce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "301\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-11 20:21:14.782134: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 994076000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7767/7767 [==============================] - 180s 22ms/step - loss: 0.2525 - mse: 244.7343 - mae: 11.8783 - mape: 148964368.0000 - accuracy: 4.0238e-05\n",
            "Epoch 2/3\n",
            "7767/7767 [==============================] - 176s 23ms/step - loss: 0.1178 - mse: 130.5561 - mae: 8.4642 - mape: 68774712.0000 - accuracy: 2.0119e-05\n",
            "Epoch 3/3\n",
            "7767/7767 [==============================] - 175s 23ms/step - loss: 0.1048 - mse: 115.5286 - mae: 7.8960 - mape: 59585124.0000 - accuracy: 4.0238e-05\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.gitignore\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/obfuscate.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/TokenizerManager.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/test.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.DS_Store\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/main.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.git\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/PythonProcessing.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/CodeSimilarization.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/TokenProcessing.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/CodeSimilarization.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/PythonProcessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/SupportingClasses/__pycache__/TokenProcessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPTokenizer.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/obfuscate.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPPreprocessing.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/TokenizerManager.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CLPTokenizer.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/Grader.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/Grader.cpython-310.pyc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "tar: Removing leading `/' from member names\n",
            "tar: Removing leading `/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../' from member names\n"
          ]
        }
      ],
      "source": [
        "#if (not gpu_detected):\n",
        "#    print(\"GPU not detected, using CPU\")\n",
        "\n",
        "number_of_tokens = len(tokenizer.word_index) + 1\n",
        "print (number_of_tokens)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(number_of_tokens, 64, input_length=maxLen))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Reshape((1, 64)))\n",
        "model.add(layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.38479930887149405))\n",
        "model.add(layers.Bidirectional(LSTM(32)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='Adam', metrics=['mse', 'mae', 'mape', 'accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert inputs to numpy arrays\n",
        "token_code = np.array(combinedDF[\"tokenCode\"].tolist())\n",
        "#token_group_code = np.array(combinedDF[\"tokenGroupCode\"].tolist())\n",
        "file_grade = np.array(combinedDF[\"fileGrade\"].tolist())\n",
        "\n",
        "hist = model.fit(token_code, file_grade, epochs=3, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "#Save the model in timestamp folder and with tokenizer\n",
        "timestamp = str(pd.Timestamp.now()).replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
        "if not os.path.exists(modelOutputPath):\n",
        "    os.mkdir(modelOutputPath)\n",
        "if not os.path.exists(modelOutputPath + \"/\" + timestamp):\n",
        "    os.mkdir(modelOutputPath + \"/\" + timestamp)\n",
        "model.save(modelOutputPath + \"/\" + timestamp + \"/model.h5\")\n",
        "with open(modelOutputPath + \"/\" + timestamp + \"/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "#make an archaive of the SCA-Tokenizer Folder\n",
        "#get CWD\n",
        "cwd = os.getcwd()\n",
        "if os.path.exists(os.path.join(cwd, \"SCA-Tokenizer\")):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/SCA-Tokenizer.tar.gz\\\" \" + os.path.join(cwd, \"SCA-Tokenizer\"))\n",
        "\n",
        "#Save the AutoGrader Folder\n",
        "autoGraderDir = os.path.join(cwd, \"../Auto-Grader/\")\n",
        "if os.path.exists(autoGraderDir):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/Auto-Grader.tar.gz\\\" \" + autoGraderDir)\n",
        "\n",
        "#Save tokenizedGroupData\n",
        "#with open(modelOutputPath + \"/\" + timestamp + \"/tokenizedGroupDataframe.pkl\", \"wb\") as f:\n",
        "#    pickle.dump(tokenizedGroupData, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
