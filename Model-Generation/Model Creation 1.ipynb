{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "+ dataset_path=/home/jaredrussell/CPPDataset/\n",
            "+ output_path=/home/jaredrussell/CPPMLGen/\n",
            "++ date +%s\n",
            "+ datevar=1701475130\n",
            "+ grades_path=/home/jaredrussell/CPPMLGen/Grades/\n",
            "+ grades_path=/home/jaredrussell/CPPMLGen/Grades/1701475130\n",
            "+ token_path=/home/jaredrussell/CPPMLGen/Tokens/\n",
            "+ token_path=/home/jaredrussell/CPPMLGen/Tokens/1701475130\n",
            "+ catigory_input=/home/jaredrussell/CPPMLGen/Tokens/1701475130\n",
            "+ catigory_path=/home/jaredrussell/CPPMLGen/Tokens/1701475130\n",
            "+ python3 ./Auto-Grader/Grader.py -i /home/jaredrussell/CPPDataset/ -o /home/jaredrussell/CPPMLGen/Grades/1701475130\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Files |█████████-----------------------------------------| 18.7% Complete\r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/Model Creation 1.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/Model%20Creation%201.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msubprocess\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/Model%20Creation%201.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m command \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./genModelPrereqs.sh /home/jaredrussell/CPPDataset/ /home/jaredrussell/CPPMLGen/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/Model%20Creation%201.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m subprocess\u001b[39m.\u001b[39;49mrun(command, shell\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, cwd\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/home/jaredrussell/gitRepos/SCA-ML-grade/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39mpopenargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39;49mcommunicate(\u001b[39minput\u001b[39;49m, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    506\u001b[0m     \u001b[39mexcept\u001b[39;00m TimeoutExpired \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[39m.\u001b[39mkill()\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m   1147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[39m=\u001b[39m _time() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[39m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[39m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[39m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[39mbreak\u001b[39;00m  \u001b[39m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_wait(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   1960\u001b[0m \u001b[39m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[39m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[39m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m pid \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid:\n",
            "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mwaitpid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpid, wait_flags)\n\u001b[1;32m   1918\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[39m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[39m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[39m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpid\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Files |████████████████████████████████████████████------| 89.4% Complete\r"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "command = \"./genModelPrereqs.sh /home/jaredrussell/CPPDataset/ /home/jaredrussell/CPPMLGen/\"\n",
        "subprocess.run(command, shell=True, cwd=\"/home/jaredrussell/gitRepos/SCA-ML-grade/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BFlZYHLP1AyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def homePath(path):\n",
        "    if path[0] == \"~\":\n",
        "        return os.path.join(os.path.expanduser(\"~\"), path.strip(\"~/\"))\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "#Model Creation 1\n",
        "#Figure out how to do versioning effectively\n",
        "individualInput = False\n",
        "combinedInputPath = \"/home/jaredrussell/CPPMLGen\"\n",
        "#combinedInputPath = \"/home/jaredrussell/CPPMLGenMini\"\n",
        "pathToTokenizedData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Tokenizer\\\\\"\n",
        "pathToGradeData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Grader\\\\\"\n",
        "GradesTokensName = \"\"\n",
        "\n",
        "\n",
        "if not individualInput:\n",
        "    pathToTokenizedData = os.path.join(combinedInputPath, \"Tokens/\")\n",
        "    pathToGradeData = os.path.join(combinedInputPath, \"Grades/\")\n",
        "\n",
        "if GradesTokensName == \"\":\n",
        "    #Use newest folder for each\n",
        "\n",
        "    #Get the newest folder for the tokens\n",
        "    tokensFolders = os.listdir(pathToTokenizedData)\n",
        "    tokensFolders.sort()\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData , tokensFolders[-1])\n",
        "\n",
        "    #Get the newest folder for the grades\n",
        "    gradesFolders = os.listdir(pathToGradeData)\n",
        "    gradesFolders.sort()\n",
        "    pathToGradeData = os.path.join(pathToGradeData,  gradesFolders[-1])\n",
        "else:\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData, GradesTokensName)\n",
        "    pathToGradeData = os.path.join(pathToGradeData, GradesTokensName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelOutputPath = \"/home/jaredrussell/CPPMLGen/Models\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nLHjdZFH05pS",
        "outputId": "1016b38b-0f18-42e3-8e28-1e03019b46a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0       [10, 1, 10, 10, 1, 10, 279, 3, 1, 2, 1, 1, 1, ...   \n",
            "1                                                           \n",
            "2                                                           \n",
            "3                                                           \n",
            "4       [10, 1, 10, 10, 1, 10, 279, 3, 1, 2, 1, 1, 1, ...   \n",
            "...                                                   ...   \n",
            "116555                                                      \n",
            "116556                                                      \n",
            "116557                                                      \n",
            "116558  [29, 1, 29, 1, 46, 1, 1, 1, 1, 1, 3, 1, 1, 1, ...   \n",
            "116559                                                      \n",
            "\n",
            "                                                     Path  \n",
            "0                                3385567/RBHook.cpp/0.cpp  \n",
            "1                                3385567/RBHook.cpp/4.cpp  \n",
            "2                                3385567/RBHook.cpp/2.cpp  \n",
            "3                                3385567/RBHook.cpp/5.cpp  \n",
            "4                                3385567/RBHook.cpp/1.cpp  \n",
            "...                                                   ...  \n",
            "116555  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  \n",
            "116556  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  \n",
            "116557  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  \n",
            "116558  3397270/examples/90.Tools/02.Tiny_UniProg/Fuse...  \n",
            "116559  3397270/examples/23_A.Selectrix_Interface/SX20...  \n",
            "\n",
            "[116547 rows x 2 columns]\n",
            "        fileGrade                                               Path\n",
            "0       55.000000                           3385567/RBHook.cpp/0.cpp\n",
            "1       64.333333                           3385567/RBHook.cpp/1.cpp\n",
            "2       67.666667                           3385567/RBHook.cpp/2.cpp\n",
            "3       65.000000                           3385567/RBHook.cpp/3.cpp\n",
            "4       68.333333                           3385567/RBHook.cpp/4.cpp\n",
            "...           ...                                                ...\n",
            "154153  39.133333  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...\n",
            "154154  19.800000  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154155  26.466667  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154156  39.133333  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154157  19.800000  3397270/examples/90.Tools/02.Tiny_UniProg/Fuse...\n",
            "\n",
            "[154158 rows x 2 columns]\n",
            "                                                tokenCode  \\\n",
            "0       [10, 1, 10, 10, 1, 10, 279, 3, 1, 2, 1, 1, 1, ...   \n",
            "1                                                           \n",
            "2                                                           \n",
            "3                                                           \n",
            "4       [10, 1, 10, 10, 1, 10, 279, 3, 1, 2, 1, 1, 1, ...   \n",
            "...                                                   ...   \n",
            "115911                                                      \n",
            "115912                                                      \n",
            "115913                                                      \n",
            "115914                                                      \n",
            "115915  [29, 1, 29, 1, 46, 1, 1, 1, 1, 1, 3, 1, 1, 1, ...   \n",
            "\n",
            "                                                     Path  fileGrade  \n",
            "0                                3385567/RBHook.cpp/0.cpp  55.000000  \n",
            "1                                3385567/RBHook.cpp/4.cpp  68.333333  \n",
            "2                                3385567/RBHook.cpp/2.cpp  67.666667  \n",
            "3                                3385567/RBHook.cpp/5.cpp  71.666667  \n",
            "4                                3385567/RBHook.cpp/1.cpp  64.333333  \n",
            "...                                                   ...        ...  \n",
            "115911  3397270/examples/80.Modules/01.ATTiny85_Servo/...  29.800000  \n",
            "115912  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  19.800000  \n",
            "115913  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  19.800000  \n",
            "115914  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  26.466667  \n",
            "115915  3397270/examples/90.Tools/02.Tiny_UniProg/Fuse...  19.800000  \n",
            "\n",
            "[115916 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('./SCA-Tokenizer/')\n",
        "import CLPTokenizer as CLPTokenizer\n",
        "\n",
        "\n",
        "\n",
        "#Load the data\n",
        "#tokenized data is in tokenizedData.pkl, has tokenizer obj in tokenizer.json\n",
        "\n",
        "#Load the tokenizer\n",
        "with open(pathToTokenizedData + \"/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f, encoding='latin1')\n",
        "\n",
        "\n",
        "#Load the tokenized data\n",
        "with open(pathToTokenizedData + \"/tokenizedData.pkl\", \"rb\") as f:\n",
        "    tokenizedData = pickle.load(f)\n",
        "\n",
        "print (tokenizedData)\n",
        "\n",
        "#Load the grade data\n",
        "#gradeData is a dict with keys as the file names and values as the grades\n",
        "with open(pathToGradeData + \"/grades.pkl\", \"rb\") as f:\n",
        "    gradeData = pickle.load(f)\n",
        "\n",
        "print (gradeData)\n",
        "\n",
        "#load the group data\n",
        "#with open(pathToTokenizedData + \"/tokenGroupDataframe.pkl\", \"rb\") as f:\n",
        "#    tokenizedGroupData = pickle.load(f)\n",
        "\n",
        "combinedDF = pd.merge(tokenizedData, gradeData, on = \"Path\")\n",
        "#combinedDF = pd.merge(combinedDF, tokenizedGroupData, on = \"Path\")\n",
        "print (combinedDF)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jnja4-p705pU",
        "outputId": "b79f4a7d-57af-4836-e052-f2089eaec5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "5       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "6       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "...                                                   ...   \n",
            "115910  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "115911  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "115912  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "115913  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "115914  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                                     Path  fileGrade  \n",
            "1                                3385567/RBHook.cpp/4.cpp  68.333333  \n",
            "2                                3385567/RBHook.cpp/2.cpp  67.666667  \n",
            "3                                3385567/RBHook.cpp/5.cpp  71.666667  \n",
            "5                                3385567/RBHook.cpp/3.cpp  65.000000  \n",
            "6                                3385567/RBTray.cpp/4.cpp  59.210526  \n",
            "...                                                   ...        ...  \n",
            "115910  3397270/examples/80.Modules/01.ATTiny85_Servo/...  19.800000  \n",
            "115911  3397270/examples/80.Modules/01.ATTiny85_Servo/...  29.800000  \n",
            "115912  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  19.800000  \n",
            "115913  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  19.800000  \n",
            "115914  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...  26.466667  \n",
            "\n",
            "[103517 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#Padding\n",
        "\n",
        "maxLen = 500\n",
        "minLen = 100\n",
        "#get rid of the ones that are too long\n",
        "\n",
        "#print (type(combinedDF[\"tokenCode\"]))\n",
        "combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) <= maxLen -1]\n",
        "\n",
        "#shorten the ones that are too long FOR TESTING\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()[:maxLen]])\n",
        "\n",
        "#get rid of the ones that are too short DISABLED FOR TESTING\n",
        "#combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) > minLen]\n",
        "\n",
        "#Pad the sequences\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()]).tolist()\n",
        "combinedDF[\"tokenCode\"] = pad_sequences(combinedDF[\"tokenCode\"].tolist(), maxlen=maxLen, padding=\"post\", truncating=\"post\").tolist()\n",
        "#combinedDF[\"tokenGroupCode\"] = pad_sequences(combinedDF[\"tokenGroupCode\"], maxlen = maxLen, padding = \"post\", truncating = \"post\").tolist()\n",
        "\n",
        "print(combinedDF)\n",
        "\n",
        "#only use 5% of the data\n",
        "#combinedDF = combinedDF.sample(frac=0.001, random_state=1)\n",
        "\n",
        "#48590 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Nu6hyuey05pU",
        "outputId": "2d9cf582-6a4a-46ad-e21c-e84a8f2eecce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "301\n",
            "Epoch 1/3\n",
            "3235/3235 [==============================] - 47s 13ms/step - loss: 0.8005 - mse: 639.4360 - mae: 21.3209 - mape: 380942048.0000 - accuracy: 4.5403e-04\n",
            "Epoch 2/3\n",
            "3235/3235 [==============================] - 42s 13ms/step - loss: 0.5258 - mse: 483.8854 - mae: 18.1284 - mape: 225555312.0000 - accuracy: 0.0031\n",
            "Epoch 3/3\n",
            "3235/3235 [==============================] - 42s 13ms/step - loss: 0.4954 - mse: 464.0258 - mae: 17.4660 - mape: 199111856.0000 - accuracy: 0.0038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.gitignore\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/TokenizerManager.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.DS_Store\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/main.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.git\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPTokenizer.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/TokenizerManager.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CLPTokenizer.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/Grader.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/Grader.cpython-310.pyc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "tar: Removing leading `/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../' from member names\n"
          ]
        }
      ],
      "source": [
        "#if (not gpu_detected):\n",
        "#    print(\"GPU not detected, using CPU\")\n",
        "\n",
        "number_of_tokens = len(tokenizer.word_index) + 1\n",
        "print (number_of_tokens)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(number_of_tokens, 64, input_length=maxLen))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Reshape((1, 64)))\n",
        "model.add(layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.38479930887149405))\n",
        "model.add(layers.Bidirectional(LSTM(32)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='Adam', metrics=['mse', 'mae', 'mape', 'accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert inputs to numpy arrays\n",
        "token_code = np.array(combinedDF[\"tokenCode\"].tolist())\n",
        "#token_group_code = np.array(combinedDF[\"tokenGroupCode\"].tolist())\n",
        "file_grade = np.array(combinedDF[\"fileGrade\"].tolist())\n",
        "\n",
        "hist = model.fit(token_code, file_grade, epochs=3, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "#Save the model in timestamp folder and with tokenizer\n",
        "timestamp = str(pd.Timestamp.now()).replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
        "if not os.path.exists(modelOutputPath):\n",
        "    os.mkdir(modelOutputPath)\n",
        "if not os.path.exists(modelOutputPath + \"/\" + timestamp):\n",
        "    os.mkdir(modelOutputPath + \"/\" + timestamp)\n",
        "model.save(modelOutputPath + \"/\" + timestamp + \"/model.h5\")\n",
        "with open(modelOutputPath + \"/\" + timestamp + \"/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "#make an archaive of the SCA-Tokenizer Folder\n",
        "#get CWD\n",
        "cwd = os.getcwd()\n",
        "if os.path.exists(os.path.join(cwd, \"SCA-Tokenizer\")):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/SCA-Tokenizer.tar.gz\\\" \" + os.path.join(cwd, \"SCA-Tokenizer\"))\n",
        "\n",
        "#Save the AutoGrader Folder\n",
        "autoGraderDir = os.path.join(cwd, \"../Auto-Grader/\")\n",
        "if os.path.exists(autoGraderDir):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/Auto-Grader.tar.gz\\\" \" + autoGraderDir)\n",
        "\n",
        "#Save tokenizedGroupData\n",
        "#with open(modelOutputPath + \"/\" + timestamp + \"/tokenizedGroupDataframe.pkl\", \"wb\") as f:\n",
        "#    pickle.dump(tokenizedGroupData, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
