{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BFlZYHLP1AyZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def homePath(path):\n",
        "    if path[0] == \"~\":\n",
        "        return os.path.join(os.path.expanduser(\"~\"), path.strip(\"~/\"))\n",
        "    else:\n",
        "        return path\n",
        "\n",
        "#Model Creation 1\n",
        "#Figure out how to do versioning effectively\n",
        "individualInput = False\n",
        "combinedInputPath = \"/home/jaredrussell/CPPMLGen\"\n",
        "#combinedInputPath = \"/home/jaredrussell/CPPMLGenMini\"\n",
        "pathToTokenizedData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Tokenizer\\\\\"\n",
        "pathToGradeData = \"C:\\\\Users\\\\mcall\\\\OneDrive\\\\Desktop\\\\DummyOutput\\\\Grader\\\\\"\n",
        "GradesTokensName = \"\"\n",
        "\n",
        "\n",
        "if not individualInput:\n",
        "    pathToTokenizedData = os.path.join(combinedInputPath, \"Tokens/\")\n",
        "    pathToGradeData = os.path.join(combinedInputPath, \"Grades/\")\n",
        "\n",
        "if GradesTokensName == \"\":\n",
        "    #Use newest folder for each\n",
        "\n",
        "    #Get the newest folder for the tokens\n",
        "    tokensFolders = os.listdir(pathToTokenizedData)\n",
        "    tokensFolders.sort()\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData , tokensFolders[-1])\n",
        "\n",
        "    #Get the newest folder for the grades\n",
        "    gradesFolders = os.listdir(pathToGradeData)\n",
        "    gradesFolders.sort()\n",
        "    pathToGradeData = os.path.join(pathToGradeData,  gradesFolders[-1])\n",
        "else:\n",
        "    pathToTokenizedData = os.path.join(pathToTokenizedData, GradesTokensName)\n",
        "    pathToGradeData = os.path.join(pathToGradeData, GradesTokensName)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "modelOutputPath = \"/home/jaredrussell/CPPMLGen/Models\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nLHjdZFH05pS",
        "outputId": "1016b38b-0f18-42e3-8e28-1e03019b46a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-29 21:07:18.558670: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-29 21:07:18.598696: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-29 21:07:18.598733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-29 21:07:18.599977: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-29 21:07:18.605518: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-29 21:07:18.605890: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-29 21:07:19.359609: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                tokenCode  \\\n",
            "0        9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "1        9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "2        9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "3        9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "4        9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "...                                                   ...   \n",
            "117086   45 20 550 65 31 46 35 28 28 9 119 706 690 65 ...   \n",
            "117087   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "117088   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "117089   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "117093   45 15 15 15 15 15 15 15 81 15 15 15 133 283 3...   \n",
            "\n",
            "                                                     Path  \n",
            "0                                3385567/RBHook.cpp/4.cpp  \n",
            "1                                3385567/RBHook.cpp/0.cpp  \n",
            "2                                3385567/RBHook.cpp/5.cpp  \n",
            "3                                3385567/RBHook.cpp/2.cpp  \n",
            "4                                3385567/RBHook.cpp/1.cpp  \n",
            "...                                                   ...  \n",
            "117086  3397270/examples/80.Modules/01.ATTiny85_Servo/...  \n",
            "117087  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  \n",
            "117088  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  \n",
            "117089  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  \n",
            "117093  3397270/examples/23_A.Selectrix_Interface/SX20...  \n",
            "\n",
            "[74859 rows x 2 columns]\n",
            "        fileGrade                                               Path\n",
            "0       55.000000                           3385567/RBHook.cpp/0.cpp\n",
            "1       64.333333                           3385567/RBHook.cpp/1.cpp\n",
            "2       67.666667                           3385567/RBHook.cpp/2.cpp\n",
            "3       65.000000                           3385567/RBHook.cpp/3.cpp\n",
            "4       68.333333                           3385567/RBHook.cpp/4.cpp\n",
            "...           ...                                                ...\n",
            "154153  39.133333  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...\n",
            "154154  19.800000  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154155  26.466667  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154156  39.133333  3397270/examples/90.Tools/02.Tiny_UniProg/ATTi...\n",
            "154157  19.800000  3397270/examples/90.Tools/02.Tiny_UniProg/Fuse...\n",
            "\n",
            "[154158 rows x 2 columns]\n",
            "                                               tokenCode  \\\n",
            "0       9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "1       9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "2       9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "3       9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "4       9 9 9 9 276 2 485 1 9 276 2 485 1 9 9 133 394...   \n",
            "...                                                  ...   \n",
            "74335   28 9 14 9 351 147 191 50 297 9 52 3 28 28 28 ...   \n",
            "74336   45 20 550 65 31 46 35 28 28 9 119 706 690 65 ...   \n",
            "74337   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "74338   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "74339   28 28 28 28 45 485 21 23 42 106 47 42 23 64 2...   \n",
            "\n",
            "                                                    Path  fileGrade  \n",
            "0                               3385567/RBHook.cpp/4.cpp  68.333333  \n",
            "1                               3385567/RBHook.cpp/0.cpp  55.000000  \n",
            "2                               3385567/RBHook.cpp/5.cpp  71.666667  \n",
            "3                               3385567/RBHook.cpp/2.cpp  67.666667  \n",
            "4                               3385567/RBHook.cpp/1.cpp  64.333333  \n",
            "...                                                  ...        ...  \n",
            "74335  3397270/examples/80.Modules/02.CharlieplexTiny...  35.800000  \n",
            "74336  3397270/examples/80.Modules/01.ATTiny85_Servo/...  19.800000  \n",
            "74337  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  19.800000  \n",
            "74338  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  39.133333  \n",
            "74339  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  26.466667  \n",
            "\n",
            "[74340 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import pickle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.text import tokenizer_from_json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "import keras.layers as layers\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append('./SCA-Tokenizer/')\n",
        "import CLPTokenizer as CLPTokenizer\n",
        "\n",
        "\n",
        "\n",
        "#Load the data\n",
        "#tokenized data is in tokenizedData.pkl, has tokenizer obj in tokenizer.json\n",
        "\n",
        "#Load the tokenizer\n",
        "with open(pathToTokenizedData + \"/tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f, encoding='latin1')\n",
        "\n",
        "\n",
        "#Load the tokenized data\n",
        "with open(pathToTokenizedData + \"/tokenizedData.pkl\", \"rb\") as f:\n",
        "    tokenizedData = pickle.load(f)\n",
        "\n",
        "print (tokenizedData)\n",
        "\n",
        "#Load the grade data\n",
        "#gradeData is a dict with keys as the file names and values as the grades\n",
        "with open(pathToGradeData + \"/grades.pkl\", \"rb\") as f:\n",
        "    gradeData = pickle.load(f)\n",
        "\n",
        "print (gradeData)\n",
        "\n",
        "#load the group data\n",
        "#with open(pathToTokenizedData + \"/tokenGroupDataframe.pkl\", \"rb\") as f:\n",
        "#    tokenizedGroupData = pickle.load(f)\n",
        "\n",
        "combinedDF = pd.merge(tokenizedData, gradeData, on = \"Path\")\n",
        "#combinedDF = pd.merge(combinedDF, tokenizedGroupData, on = \"Path\")\n",
        "print (combinedDF)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jnja4-p705pU",
        "outputId": "b79f4a7d-57af-4836-e052-f2089eaec5b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                               tokenCode  \\\n",
            "0      [9, 9, 9, 9, 276, 2, 485, 1, 9, 276, 2, 485, 1...   \n",
            "1      [9, 9, 9, 9, 276, 2, 485, 1, 9, 276, 2, 485, 1...   \n",
            "2      [9, 9, 9, 9, 276, 2, 485, 1, 9, 276, 2, 485, 1...   \n",
            "3      [9, 9, 9, 9, 276, 2, 485, 1, 9, 276, 2, 485, 1...   \n",
            "4      [9, 9, 9, 9, 276, 2, 485, 1, 9, 276, 2, 485, 1...   \n",
            "...                                                  ...   \n",
            "74335  [28, 9, 14, 9, 351, 147, 191, 50, 297, 9, 52, ...   \n",
            "74336  [45, 20, 550, 65, 31, 46, 35, 28, 28, 9, 119, ...   \n",
            "74337  [28, 28, 28, 28, 45, 485, 21, 23, 42, 106, 47,...   \n",
            "74338  [28, 28, 28, 28, 45, 485, 21, 23, 42, 106, 47,...   \n",
            "74339  [28, 28, 28, 28, 45, 485, 21, 23, 42, 106, 47,...   \n",
            "\n",
            "                                                    Path  fileGrade  \n",
            "0                               3385567/RBHook.cpp/4.cpp  68.333333  \n",
            "1                               3385567/RBHook.cpp/0.cpp  55.000000  \n",
            "2                               3385567/RBHook.cpp/5.cpp  71.666667  \n",
            "3                               3385567/RBHook.cpp/2.cpp  67.666667  \n",
            "4                               3385567/RBHook.cpp/1.cpp  64.333333  \n",
            "...                                                  ...        ...  \n",
            "74335  3397270/examples/80.Modules/02.CharlieplexTiny...  35.800000  \n",
            "74336  3397270/examples/80.Modules/01.ATTiny85_Servo/...  19.800000  \n",
            "74337  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  19.800000  \n",
            "74338  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  39.133333  \n",
            "74339  3397270/examples/90.Tools/02.Tiny_UniProg/LEDs...  26.466667  \n",
            "\n",
            "[74340 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "#Padding\n",
        "\n",
        "maxLen = 500\n",
        "minLen = 100\n",
        "#get rid of the ones that are too long\n",
        "\n",
        "#print (type(combinedDF[\"tokenCode\"]))\n",
        "#combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) <= maxLen -1]\n",
        "\n",
        "#shorten the ones that are too long FOR TESTING\n",
        "combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()[:maxLen]])\n",
        "\n",
        "#get rid of the ones that are too short DISABLED FOR TESTING\n",
        "#combinedDF = combinedDF[combinedDF[\"tokenCode\"].apply(lambda x: len(x)) > minLen]\n",
        "\n",
        "#Pad the sequences\n",
        "#combinedDF[\"tokenCode\"] = combinedDF[\"tokenCode\"].apply(lambda x: [int(i) for i in x.split()]).tolist()\n",
        "combinedDF[\"tokenCode\"] = pad_sequences(combinedDF[\"tokenCode\"].tolist(), maxlen=maxLen, padding=\"post\", truncating=\"post\").tolist()\n",
        "#combinedDF[\"tokenGroupCode\"] = pad_sequences(combinedDF[\"tokenGroupCode\"], maxlen = maxLen, padding = \"post\", truncating = \"post\").tolist()\n",
        "\n",
        "print(combinedDF)\n",
        "\n",
        "#only use 5% of the data\n",
        "#combinedDF = combinedDF.sample(frac=0.001, random_state=1)\n",
        "\n",
        "#48590 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Nu6hyuey05pU",
        "outputId": "2d9cf582-6a4a-46ad-e21c-e84a8f2eecce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "801\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "2324/2324 [==============================] - 32s 12ms/step - loss: 0.5951 - mse: 385.9896 - mae: 14.3964 - mape: 271627808.0000 - accuracy: 0.0023\n",
            "Epoch 2/3\n",
            "2324/2324 [==============================] - 29s 12ms/step - loss: 0.2223 - mse: 134.8881 - mae: 8.0203 - mape: 118693400.0000 - accuracy: 0.0052\n",
            "Epoch 3/3\n",
            "2324/2324 [==============================] - 28s 12ms/step - loss: 0.1833 - mse: 109.6337 - mae: 7.1772 - mape: 84391008.0000 - accuracy: 0.0061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.gitignore\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/TokenizerManager.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.DS_Store\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/main.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/.git\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CLPTokenizer.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/TokenizerManager.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/SCA-Tokenizer/CLPTokenizer.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/Grader.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/CaMlSupportingClasses.py\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/SupportingClasses/__pycache__/CaMlSupportingClasses.cpython-310.pyc\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/\n",
            "/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../Auto-Grader/__pycache__/Grader.cpython-310.pyc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Removing leading `/' from member names\n",
            "tar: Removing leading `/home/jaredrussell/gitRepos/SCA-ML-grade/Model-Generation/../' from member names\n"
          ]
        }
      ],
      "source": [
        "#if (not gpu_detected):\n",
        "#    print(\"GPU not detected, using CPU\")\n",
        "\n",
        "number_of_tokens = len(tokenizer.word_index) + 1\n",
        "print (number_of_tokens)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(number_of_tokens, 64, input_length=maxLen))\n",
        "model.add(layers.Conv1D(64, 3, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Reshape((1, 64)))\n",
        "model.add(layers.Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Dropout(0.38479930887149405))\n",
        "model.add(layers.Bidirectional(LSTM(32)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(loss='mean_squared_logarithmic_error', optimizer='Adam', metrics=['mse', 'mae', 'mape', 'accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert inputs to numpy arrays\n",
        "token_code = np.array(combinedDF[\"tokenCode\"].tolist())\n",
        "#token_group_code = np.array(combinedDF[\"tokenGroupCode\"].tolist())\n",
        "file_grade = np.array(combinedDF[\"fileGrade\"].tolist())\n",
        "\n",
        "hist = model.fit(token_code, file_grade, epochs=3, batch_size=32, verbose=1)\n",
        "\n",
        "\n",
        "#Save the model in timestamp folder and with tokenizer\n",
        "timestamp = str(pd.Timestamp.now()).replace(\" \", \"_\").replace(\":\", \"-\").replace(\".\", \"-\")\n",
        "if not os.path.exists(modelOutputPath):\n",
        "    os.mkdir(modelOutputPath)\n",
        "if not os.path.exists(modelOutputPath + \"/\" + timestamp):\n",
        "    os.mkdir(modelOutputPath + \"/\" + timestamp)\n",
        "model.save(modelOutputPath + \"/\" + timestamp + \"/model.h5\")\n",
        "with open(modelOutputPath + \"/\" + timestamp + \"/tokenizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "#make an archaive of the SCA-Tokenizer Folder\n",
        "#get CWD\n",
        "cwd = os.getcwd()\n",
        "if os.path.exists(os.path.join(cwd, \"SCA-Tokenizer\")):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/SCA-Tokenizer.tar.gz\\\" \" + os.path.join(cwd, \"SCA-Tokenizer\"))\n",
        "\n",
        "#Save the AutoGrader Folder\n",
        "autoGraderDir = os.path.join(cwd, \"../Auto-Grader/\")\n",
        "if os.path.exists(autoGraderDir):\n",
        "    os.system(\"tar -czvf \\\"\" + modelOutputPath + \"/\" + timestamp + \"/Auto-Grader.tar.gz\\\" \" + autoGraderDir)\n",
        "\n",
        "#Save tokenizedGroupData\n",
        "#with open(modelOutputPath + \"/\" + timestamp + \"/tokenizedGroupDataframe.pkl\", \"wb\") as f:\n",
        "#    pickle.dump(tokenizedGroupData, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
