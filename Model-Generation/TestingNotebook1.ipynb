{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "newestModel = True #If this is set to true it will use the newest model in the models folder and ignore modelToTest\n",
    "modelToTest = ''\n",
    "pathToModelFolder = '~/SPGenerations/Models/'\n",
    "pathToDataFolder = '~/SPDataset/smallDataset/S2DS245-1014/'\n",
    "#a folder inthe models folder contains the following files: model.h5, tokenizer.json commitHash.txt (For SCA-Tokenizer)\n",
    "useCurrentSCATokenizer = True #If this is set to true it will use the current SCA-Token in the SCA-Token folder and ignore the modelToTest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import math\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "\n",
    "def homePath(path):\n",
    "    return os.path.join(os.path.expanduser(\"~\"), path.strip(\"~/\"))\n",
    "\n",
    "pathToModelFolder = homePath(pathToModelFolder)\n",
    "pathToDataFolder = homePath(pathToDataFolder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using newest model: 2023-10-17 18:59:54.067245\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 400, 1024)         10240000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 398, 2048)         6293504   \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 2048)              0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 2048)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 1, 4096)           67125248  \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 4096)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 2048)              41951232  \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              4196352   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 129808385 (495.18 MB)\n",
      "Trainable params: 129808385 (495.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mcall/SPGit/Model-Generation/TestingNotebook1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.238.9.7/home/mcall/SPGit/Model-Generation/TestingNotebook1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#load tokenizer from json that used the .tokenizer.save() function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B141.238.9.7/home/mcall/SPGit/Model-Generation/TestingNotebook1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(pathToModelFolder \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m modelToTest \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/tokenizer.json\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B141.238.9.7/home/mcall/SPGit/Model-Generation/TestingNotebook1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     tokenizer \u001b[39m=\u001b[39m tokenizer_from_json(f\u001b[39m.\u001b[39;49mread)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/preprocessing/text.py:595\u001b[0m, in \u001b[0;36mtokenizer_from_json\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.preprocessing.text.tokenizer_from_json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    579\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenizer_from_json\u001b[39m(json_string):\n\u001b[1;32m    580\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Parses a JSON tokenizer configuration and returns a tokenizer instance.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \u001b[39m    Deprecated: `tf.keras.preprocessing.text.Tokenizer` does not operate on\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39m        A Keras Tokenizer instance\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 595\u001b[0m     tokenizer_config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(json_string)\n\u001b[1;32m    596\u001b[0m     config \u001b[39m=\u001b[39m tokenizer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mconfig\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    598\u001b[0m     word_counts \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(config\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mword_counts\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/json/__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[0;32m--> 339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    340\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "#File Loading\n",
    "\n",
    "if newestModel:\n",
    "    models = os.listdir(pathToModelFolder)\n",
    "    models.sort()\n",
    "    modelToTest = models[-1]\n",
    "    print(\"Using newest model: \" + modelToTest)\n",
    "else:\n",
    "    print(\"Using model: \" + modelToTest)\n",
    "model = load_model(pathToModelFolder + '/' +modelToTest + '/model.h5')\n",
    "print (model.summary())\n",
    "\n",
    "#load tokenizer from json that used the .tokenizer.save() function\n",
    "with open(pathToModelFolder + '/' + modelToTest + '/tokenizer.json') as f:\n",
    "    tokenizer = tokenizer_from_json(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToSCATokenizerFolder = os.getcwd() + '/SCA-Tokenizer/'\n",
    "if (not useCurrentSCATokenizer):\n",
    "    #TODO: Configure Git clone on the commit hash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
